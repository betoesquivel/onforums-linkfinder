{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from testdataextractor.testdataextractor.extractor import Extractor\n",
    "from summpy.summpy import lexrank\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "203 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "290 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "178 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "170 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "184 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "278 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "156 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "196 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "178 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testarticles = [1957284403,1965754064,233465322,283147769,362778020,37793736,389321649,540607195,60134403,887344770, ]\n",
    "all_articles = []\n",
    "all_sets_sentences = []\n",
    "for art in testarticles:\n",
    "    ext = Extractor(\"../test_data/{0}.ofs.gold.xml\".format(art))\n",
    "    article = ext.extract()\n",
    "    all_articles.append(article)\n",
    "    df_article = pd.DataFrame.from_dict(article['sentences'], orient='index')\n",
    "    sorted_indexes = [ \"s{0}\".format(x) for x in range(len(article['sentences'].values()))]\n",
    "    sentences = list(df_article.ix[sorted_indexes, 'text'])\n",
    "    print len(sentences), \"sentences extracted.\"\n",
    "    if df_article.ix['s2', 'text'] == sentences[2]:\n",
    "        print \"Extracted list of sentences is in a proper order.\"\n",
    "        all_sets_sentences.append(sentences)\n",
    "    else:\n",
    "        print \"Extracted list of sentences is unordered.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data into lexrank\n",
    "The summpy MIT licensed repo used for this task, returns a tuple with a dictionary with sentences index + score, and the similarity_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ranked_sentences = []\n",
    "all_matrxs = []\n",
    "for sentences in all_sets_sentences:\n",
    "    ranked_sentences, similarity_mtrx = lexrank.lexrank(sentences)\n",
    "    all_ranked_sentences.append(ranked_sentences)\n",
    "    all_matrxs.append(similarity_mtrx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ranked_dfs = []\n",
    "all_similarity_dfs = []\n",
    "for ranked_sentences, similarity_mtrx in zip(all_ranked_sentences, all_matrxs):\n",
    "    df_ranked_sentences = pd.DataFrame.from_dict(ranked_sentences,\n",
    "                                                 orient='index')\n",
    "    df_similarity_mtrx = pd.DataFrame(similarity_mtrx)\n",
    "    all_ranked_dfs.append(df_ranked_sentences)\n",
    "    all_similarity_dfs.append(df_similarity_mtrx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 top sentences and 58 bottom_sentences\n",
      "146 top sentences and 57 bottom_sentences\n",
      "205 top sentences and 85 bottom_sentences\n",
      "124 top sentences and 54 bottom_sentences\n",
      "115 top sentences and 55 bottom_sentences\n",
      "125 top sentences and 59 bottom_sentences\n",
      "189 top sentences and 89 bottom_sentences\n",
      "107 top sentences and 49 bottom_sentences\n",
      "135 top sentences and 61 bottom_sentences\n",
      "117 top sentences and 61 bottom_sentences\n"
     ]
    }
   ],
   "source": [
    "all_top_sents = []\n",
    "all_bot_sents = []\n",
    "for df_ranked_sentences in all_ranked_dfs:\n",
    "    mean_score = df_ranked_sentences.mean(axis=0)\n",
    "    min_score = df_ranked_sentences.min(axis=0)\n",
    "    tresh = mean_score*.90 + min_score*.1\n",
    "    top_sent = list(df_ranked_sentences[df_ranked_sentences > tresh].dropna().index)\n",
    "    bottom_sent = list(df_ranked_sentences[df_ranked_sentences < tresh].dropna().index)\n",
    "    all_top_sents.append(top_sent)\n",
    "    all_bot_sents.append(bottom_sent)\n",
    "    print \"{0} top sentences and {1} bottom_sentences\".format(len(top_sent), len(bottom_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with link information\n",
    "* How many of the links are made entirely of top sentences?\n",
    "* How many are made of top and bottom sentences? \n",
    "* How many of just bottom sentences?\n",
    "\n",
    "Answering these questions will probably allow me to know if the saliency of these sentences given by lexrank is a good feature for finding links, or if it is just useful for finding the most important ones and that's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP: 0.435714285714, BOTTOM: 0.1\n",
      "MIXED: 0.464285714286, AT LEAST ONE TOP: 0.9\n",
      "\n",
      "\n",
      "TOP: 0.5, BOTTOM: 0.0882352941176\n",
      "MIXED: 0.411764705882, AT LEAST ONE TOP: 0.911764705882\n",
      "\n",
      "\n",
      "TOP: 0.48743718593, BOTTOM: 0.100502512563\n",
      "MIXED: 0.412060301508, AT LEAST ONE TOP: 0.899497487437\n",
      "\n",
      "\n",
      "TOP: 0.489130434783, BOTTOM: 0.0869565217391\n",
      "MIXED: 0.423913043478, AT LEAST ONE TOP: 0.913043478261\n",
      "\n",
      "\n",
      "TOP: 0.556962025316, BOTTOM: 0.0886075949367\n",
      "MIXED: 0.354430379747, AT LEAST ONE TOP: 0.911392405063\n",
      "\n",
      "\n",
      "TOP: 0.0, BOTTOM: 0.0\n",
      "MIXED: 1.0, AT LEAST ONE TOP: 1.0\n",
      "\n",
      "\n",
      "TOP: 0.6, BOTTOM: 0.04\n",
      "MIXED: 0.36, AT LEAST ONE TOP: 0.96\n",
      "\n",
      "\n",
      "TOP: 0.538461538462, BOTTOM: 0.115384615385\n",
      "MIXED: 0.346153846154, AT LEAST ONE TOP: 0.884615384615\n",
      "\n",
      "\n",
      "TOP: 0.357142857143, BOTTOM: 0.214285714286\n",
      "MIXED: 0.428571428571, AT LEAST ONE TOP: 0.785714285714\n",
      "\n",
      "\n",
      "TOP: 0.5, BOTTOM: 0.0\n",
      "MIXED: 0.5, AT LEAST ONE TOP: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for top_sent, bottom_sent, article in zip(all_top_sents, all_bot_sents, all_articles):\n",
    "    top_sent_set = { \"s{0}\".format(s) for s in top_sent }\n",
    "    bot_sent_set = { \"s{0}\".format(s) for s in bottom_sent }\n",
    "\n",
    "    both_top = 0\n",
    "    one_top = 0\n",
    "    both_bottom = 0\n",
    "    other = 0\n",
    "    link_dicts = article['links'].values()\n",
    "    total = len(link_dicts)*1.0\n",
    "    for l in link_dicts:\n",
    "        s_art = l['art_sentence']\n",
    "        s_com = l['com_sentence']\n",
    "        if s_art in top_sent_set and s_com in top_sent_set:\n",
    "            both_top += 1\n",
    "            one_top += 1\n",
    "        elif s_art in bot_sent_set and s_com in bot_sent_set:\n",
    "            both_bottom += 1\n",
    "        else:\n",
    "            other += 1\n",
    "            one_top += 1\n",
    "\n",
    "    print \"TOP: {0}, BOTTOM: {1}\\nMIXED: {2}, AT LEAST ONE TOP: {3}\\n\\n\"\\\n",
    "    .format(both_top/total,both_bottom/total,other/total, one_top/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can be concluded, that most of the times, links contain at least one top ranked sentence.\n",
    "This means that this can be used to just classify pairs that contain at least one top ranked sentence. Top ranked sentences are the ones that are above a certain treshold which depends on the min value and the mean value. \n",
    "\n",
    "## So now I calculate the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7656\n",
      "8322\n",
      "17425\n",
      "6696\n",
      "6325\n",
      "7375\n",
      "16821\n",
      "5243\n",
      "8235\n",
      "7137\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "all_pairs = []\n",
    "for top_sent, bottom_sent in zip(all_top_sents, all_bot_sents):\n",
    "    top_sent_set = { \"s{0}\".format(s) for s in top_sent }\n",
    "    bot_sent_set = { \"s{0}\".format(s) for s in bottom_sent }\n",
    "    \n",
    "    pairs = list(product(top_sent_set, bot_sent_set))\n",
    "    all_pairs.append(pairs)\n",
    "    print len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I can further prune this list\n",
    "Remove all pairs that have the same comment, or are both from the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7019\n",
      "7632\n",
      "16735\n",
      "6399\n",
      "6185\n",
      "7164\n",
      "15950\n",
      "4960\n",
      "7856\n",
      "6808\n"
     ]
    }
   ],
   "source": [
    "all_pruned_pairs = []\n",
    "for index, pairs in enumerate(all_pairs):\n",
    "    art = all_articles[index]\n",
    "    sents = art['sentences']\n",
    "    pruned = [p for p in pairs \n",
    "              if sents[p[0]].get('comment', 'none') != \n",
    "                 sents[p[1]].get('comment', 'none')]\n",
    "    all_pruned_pairs.append(pruned)\n",
    "    print len(pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
