{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from testdataextractor.testdataextractor.extractor import Extractor\n",
    "from summpy.summpy import lexrank\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n",
      "190 sentences extracted.\n",
      "Extracted list of sentences is in a proper order.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testarticles = [1957284403,1965754064,233465322,283147769,362778020,37793736,389321649,540607195,60134403,887344770, ]\n",
    "all_articles = []\n",
    "all_sets_sentences = []\n",
    "for art in testarticles:\n",
    "    ext = Extractor(\"../test_data/{0}.ofs.gold.xml\".format(art))\n",
    "    article = ext.extract()\n",
    "    all_articles.append(article)\n",
    "    df_article = pd.DataFrame.from_dict(article['sentences'], orient='index')\n",
    "    sorted_indexes = [ \"s{0}\".format(x) for x in range(190)]\n",
    "    sentences = list(df_article.ix[sorted_indexes, 'text'])\n",
    "    print len(sentences), \"sentences extracted.\"\n",
    "    if df_article.ix['s2', 'text'] == sentences[2]:\n",
    "        print \"Extracted list of sentences is in a proper order.\"\n",
    "        all_sets_sentences.append(sentences)\n",
    "    else:\n",
    "        print \"Extracted list of sentences is unordered.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data into lexrank\n",
    "The summpy MIT licensed repo used for this task, returns a tuple with a dictionary with sentences index + score, and the similarity_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0ecfa8c9ec40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_matrxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sets_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mranked_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_mtrx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mall_ranked_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mall_matrxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_mtrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/betoesquivel/workspace/groupproject/linkfinder/summpy/summpy/lexrank.pyc\u001b[0m in \u001b[0;36mlexrank\u001b[0;34m(sentences, continuous, sim_threshold, alpha, use_divrank, divrank_alpha)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0msent_tf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_segmenter_ja\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0msent_tf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/betoesquivel/workspace/groupproject/linkfinder/summpy/summpy/misc/janome_segmenter.pyc\u001b[0m in \u001b[0;36mword_segmenter_ja\u001b[0;34m(sent, node_filter, node2word)\u001b[0m\n\u001b[1;32m     50\u001b[0m def word_segmenter_ja(sent, node_filter=not_stopword,\n\u001b[1;32m     51\u001b[0m                       node2word=node2norm_word):\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_decode_janome_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnode_filter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/betoesquivel/Envs/ML/lib/python2.7/site-packages/janome/tokenizer.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mlattice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLattice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msys_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "all_ranked_sentences = []\n",
    "all_matrxs = []\n",
    "for sentences in all_sets_sentences:\n",
    "    ranked_sentences, similarity_mtrx = lexrank.lexrank(sentences)\n",
    "    all_ranked_sentences.append(ranked_sentences)\n",
    "    all_matrxs.append(similarity_mtrx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ranked_sentences = pd.DataFrame.from_dict(ranked_sentences,\n",
    "                                             orient='index')\n",
    "df_similarity_mtrx = pd.DataFrame(similarity_mtrx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_score = df_ranked_sentences.mean(axis=0)\n",
    "top_sent = list(df_ranked_sentences[df_ranked_sentences > mean_score].dropna().index)\n",
    "bottom_sent = list(df_ranked_sentences[df_ranked_sentences < mean_score].dropna().index)\n",
    "\n",
    "print \"{0} top sentences and {1} bottom_sentences\".format(len(top_sent), len(bottom_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with link information\n",
    "* How many of the links are made entirely of top sentences?\n",
    "* How many are made of top and bottom sentences? \n",
    "* How many of just bottom sentences?\n",
    "\n",
    "Answering these questions will probably allow me to know if the saliency of these sentences given by lexrank is a good feature for finding links, or if it is just useful for finding the most important ones and that's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_sent_set = { \"s{0}\".format(s) for s in top_sent }\n",
    "bot_sent_set = { \"s{0}\".format(s) for s in bottom_sent }\n",
    "\n",
    "both_top = 0\n",
    "one_top = 0\n",
    "both_bottom = 0\n",
    "other = 0\n",
    "link_dicts = article['links'].values()\n",
    "total = len(link_dicts)*1.0\n",
    "for l in link_dicts:\n",
    "    s_art = l['art_sentence']\n",
    "    s_com = l['com_sentence']\n",
    "    if s_art in top_sent_set and s_com in top_sent_set:\n",
    "        both_top += 1\n",
    "        one_top += 1\n",
    "    elif s_art in bot_sent_set and s_com in bot_sent_set:\n",
    "        both_bottom += 1\n",
    "    else:\n",
    "        other += 1\n",
    "        one_top += 1\n",
    "\n",
    "print \"TOP: {0}, BOTTOM: {1}, MIXED: {2}, AT LEAST ONE TOP: {3}\"\\\n",
    ".format(both_top/total,both_bottom/total,other/total, one_top/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Art1 = .8571\n",
    "Art2 = .7549\n",
    "Art3 = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
