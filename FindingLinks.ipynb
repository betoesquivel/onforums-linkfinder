{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from testdataextractor.testdataextractor.extractor import Extractor\n",
    "from summpy.summpy import lexrank\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  comments parsed.\n",
      "190  sentences parsed.\n",
      "140  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "50  comments parsed.\n",
      "203  sentences parsed.\n",
      "102  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "50  comments parsed.\n",
      "290  sentences parsed.\n",
      "199  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "49  comments parsed.\n",
      "178  sentences parsed.\n",
      "92  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "50  comments parsed.\n",
      "170  sentences parsed.\n",
      "79  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "50  comments parsed.\n",
      "184  sentences parsed.\n",
      "1  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "49  comments parsed.\n",
      "278  sentences parsed.\n",
      "25  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "49  comments parsed.\n",
      "156  sentences parsed.\n",
      "26  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "50  comments parsed.\n",
      "196  sentences parsed.\n",
      "14  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n",
      "49  comments parsed.\n",
      "178  sentences parsed.\n",
      "2  links parsed.\n",
      "Extracted list of sentences is in a proper order.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testarticles = [1957284403,1965754064,233465322,283147769,362778020,37793736,389321649,540607195,60134403,887344770, ]\n",
    "all_articles = []\n",
    "all_sets_sentences = []\n",
    "for art in testarticles:\n",
    "    ext = Extractor(\"../test_data/{0}.ofs.gold.xml\".format(art))\n",
    "    article = ext.extract(verbose=True)\n",
    "    all_articles.append(article)\n",
    "    df_article = pd.DataFrame.from_dict(article['sentences'], orient='index')\n",
    "    sorted_indexes = [ \"s{0}\".format(x) for x in range(len(article['sentences'].values()))]\n",
    "    sentences = list(df_article.ix[sorted_indexes, 'text'])\n",
    "    if df_article.ix['s2', 'text'] == sentences[2]:\n",
    "        print \"Extracted list of sentences is in a proper order.\"\n",
    "        all_sets_sentences.append(sentences)\n",
    "    else:\n",
    "        print \"Extracted list of sentences is unordered.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data into lexrank\n",
    "The summpy MIT licensed repo used for this task, returns a tuple with a dictionary with sentences index + score, and the similarity_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ranked_sentences = []\n",
    "all_matrxs = []\n",
    "for sentences in all_sets_sentences:\n",
    "    ranked_sentences, similarity_mtrx = lexrank.lexrank(sentences)\n",
    "    all_ranked_sentences.append(ranked_sentences)\n",
    "    all_matrxs.append(similarity_mtrx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ranked_dfs = []\n",
    "all_similarity_dfs = []\n",
    "for ranked_sentences, similarity_mtrx in zip(all_ranked_sentences, all_matrxs):\n",
    "    df_ranked_sentences = pd.DataFrame.from_dict(ranked_sentences,\n",
    "                                                 orient='index')\n",
    "    df_similarity_mtrx = pd.DataFrame(similarity_mtrx)\n",
    "    all_ranked_dfs.append(df_ranked_sentences)\n",
    "    all_similarity_dfs.append(df_similarity_mtrx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 top sentences and 58 bottom_sentences\n",
      "146 top sentences and 57 bottom_sentences\n",
      "205 top sentences and 85 bottom_sentences\n",
      "124 top sentences and 54 bottom_sentences\n",
      "115 top sentences and 55 bottom_sentences\n",
      "125 top sentences and 59 bottom_sentences\n",
      "189 top sentences and 89 bottom_sentences\n",
      "107 top sentences and 49 bottom_sentences\n",
      "135 top sentences and 61 bottom_sentences\n",
      "117 top sentences and 61 bottom_sentences\n"
     ]
    }
   ],
   "source": [
    "all_top_sents = []\n",
    "all_bot_sents = []\n",
    "for df_ranked_sentences in all_ranked_dfs:\n",
    "    mean_score = df_ranked_sentences.mean(axis=0)\n",
    "    min_score = df_ranked_sentences.min(axis=0)\n",
    "    tresh = mean_score*.90 + min_score*.1\n",
    "    top_sent = list(df_ranked_sentences[df_ranked_sentences > tresh].dropna().index)\n",
    "    bottom_sent = list(df_ranked_sentences[df_ranked_sentences < tresh].dropna().index)\n",
    "    all_top_sents.append(top_sent)\n",
    "    all_bot_sents.append(bottom_sent)\n",
    "    print \"{0} top sentences and {1} bottom_sentences\".format(len(top_sent), len(bottom_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with link information\n",
    "* How many of the links are made entirely of top sentences?\n",
    "* How many are made of top and bottom sentences? \n",
    "* How many of just bottom sentences?\n",
    "\n",
    "Answering these questions will probably allow me to know if the saliency of these sentences given by lexrank is a good feature for finding links, or if it is just useful for finding the most important ones and that's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP: 0.435714285714, BOTTOM: 0.1\n",
      "MIXED: 0.464285714286, AT LEAST ONE TOP: 0.9\n",
      "\n",
      "\n",
      "TOP: 0.5, BOTTOM: 0.0882352941176\n",
      "MIXED: 0.411764705882, AT LEAST ONE TOP: 0.911764705882\n",
      "\n",
      "\n",
      "TOP: 0.48743718593, BOTTOM: 0.100502512563\n",
      "MIXED: 0.412060301508, AT LEAST ONE TOP: 0.899497487437\n",
      "\n",
      "\n",
      "TOP: 0.489130434783, BOTTOM: 0.0869565217391\n",
      "MIXED: 0.423913043478, AT LEAST ONE TOP: 0.913043478261\n",
      "\n",
      "\n",
      "TOP: 0.556962025316, BOTTOM: 0.0886075949367\n",
      "MIXED: 0.354430379747, AT LEAST ONE TOP: 0.911392405063\n",
      "\n",
      "\n",
      "TOP: 0.0, BOTTOM: 0.0\n",
      "MIXED: 1.0, AT LEAST ONE TOP: 1.0\n",
      "\n",
      "\n",
      "TOP: 0.6, BOTTOM: 0.04\n",
      "MIXED: 0.36, AT LEAST ONE TOP: 0.96\n",
      "\n",
      "\n",
      "TOP: 0.538461538462, BOTTOM: 0.115384615385\n",
      "MIXED: 0.346153846154, AT LEAST ONE TOP: 0.884615384615\n",
      "\n",
      "\n",
      "TOP: 0.357142857143, BOTTOM: 0.214285714286\n",
      "MIXED: 0.428571428571, AT LEAST ONE TOP: 0.785714285714\n",
      "\n",
      "\n",
      "TOP: 0.5, BOTTOM: 0.0\n",
      "MIXED: 0.5, AT LEAST ONE TOP: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for top_sent, bottom_sent, article in zip(all_top_sents, all_bot_sents, all_articles):\n",
    "    top_sent_set = { \"s{0}\".format(s) for s in top_sent }\n",
    "    bot_sent_set = { \"s{0}\".format(s) for s in bottom_sent }\n",
    "\n",
    "    both_top = 0\n",
    "    one_top = 0\n",
    "    both_bottom = 0\n",
    "    other = 0\n",
    "    link_dicts = article['links'].values()\n",
    "    total = len(link_dicts)*1.0\n",
    "    for l in link_dicts:\n",
    "        s_art = l['art_sentence']\n",
    "        s_com = l['com_sentence']\n",
    "        if s_art in top_sent_set and s_com in top_sent_set:\n",
    "            both_top += 1\n",
    "            one_top += 1\n",
    "        elif s_art in bot_sent_set and s_com in bot_sent_set:\n",
    "            both_bottom += 1\n",
    "        else:\n",
    "            other += 1\n",
    "            one_top += 1\n",
    "\n",
    "    print \"TOP: {0}, BOTTOM: {1}\\nMIXED: {2}, AT LEAST ONE TOP: {3}\\n\\n\"\\\n",
    "    .format(both_top/total,both_bottom/total,other/total, one_top/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can be concluded, that most of the times, links contain at least one top ranked sentence.\n",
    "This means that this can be used to just classify pairs that contain at least one top ranked sentence. Top ranked sentences are the ones that are above a certain treshold which depends on the min value and the mean value. \n",
    "\n",
    "## So now I calculate the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7656\n",
      "8322\n",
      "17425\n",
      "6696\n",
      "6325\n",
      "7375\n",
      "16821\n",
      "5243\n",
      "8235\n",
      "7137\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "all_pairs = []\n",
    "for top_sent, bottom_sent in zip(all_top_sents, all_bot_sents):\n",
    "    top_sent_set = { \"s{0}\".format(s) for s in top_sent }\n",
    "    bot_sent_set = { \"s{0}\".format(s) for s in bottom_sent }\n",
    "    \n",
    "    pairs = list(product(top_sent_set, bot_sent_set))\n",
    "    all_pairs.append(pairs)\n",
    "    print len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I can further prune this list\n",
    "Remove all pairs that have the same comment, or are both from the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7019\n",
      "7632\n",
      "16735\n",
      "6399\n",
      "6185\n",
      "7164\n",
      "15950\n",
      "4960\n",
      "7856\n",
      "6808\n"
     ]
    }
   ],
   "source": [
    "all_pruned_pairs = []\n",
    "for index, pairs in enumerate(all_pairs):\n",
    "    art = all_articles[index]\n",
    "    sents = art['sentences']\n",
    "    pruned = [p for p in pairs \n",
    "              if sents[p[0]].get('comment', 'none') != \n",
    "                 sents[p[1]].get('comment', 'none')]\n",
    "    all_pruned_pairs.append(pruned)\n",
    "    print len(pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe I can do some pruning by finding semantic similarity between pairs\n",
    "I am going to need to calculate entailment and wordnet still for the pair, so I can use this features to just keep pairs that are close enough, at least for the classification.\n",
    "\n",
    "nltk has wordnet class with *path_similarity* that returns a score of path similarity saying how similar two word senses are. \n",
    "\n",
    "Finding semantic similarity is done at a word level. I am going to have to make a vector with the semantic similarity of each word. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should I do first? \n",
    "Get the most important words (a.k.a. words with largest tfidf)?\n",
    "\n",
    "Do part of speech tagging on all the words, and convert the parts of speech into the module attributes from wordnet?\n",
    "\n",
    "Find semantic similarity between a pair of sentences by averaging?\n",
    "\n",
    "\n",
    "Proposed approach to finding the links:\n",
    "\n",
    "* Vectorize the two sentences together, to find weights for the words.\n",
    "* Make an array or a set of unique words (features present in the two sentence corpus).\n",
    "* For each sentence, make a semantic similarity vector like in [[1]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.210.9942&rep=rep1&type=pdf)\n",
    "    * It is important to note that they find word similarity differently than the normal wordnet approach. Because of the similarity error explained in 3.2.1 (animal is closer to boy than teacher in wordnet, when it clearly is not. This is because depth is not taken into account when comparing the path to the other word.)\n",
    "* Calculate the cosine distance between the two similarity vectors, to get a similarity value. \n",
    "\n",
    "After this, the similarity vectors of the pair can be one of the training features for the algorithm. We can also then add sentiment analysis as a feature, maybe. Another idea for a feature is to add something from wikipedia. Maybe make two similarity vectors, one from wordnet and one from wikipedia (how many links away is a term from another term) but only with the highest tfidf scoring words in both sentences. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I should get a way to weigh words in my sentences first\n",
    "In the paper [[1]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.210.9942&rep=rep1&type=pdf), they use the Brown Corpus to get the weight for each word in a similarity vector. \n",
    "\n",
    "However, I propose using my entire sentence corpus (so the article and comments context) to get the weights. There are two ways to do this: \n",
    "1. Use TFIDF vectorizer on all the sentences and get the weight like that. I have to find a way to not remove stopwords. Stopwords are not removed by default! This is great. \n",
    "2. Use Count vectorizer on all the sentences to get a dictionary of the words and their frequencies in the whole corpus and per sentence. Have to figure out how to do this. Stopwords are not removed by default! This is great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words\n",
      "2996\n",
      "All words:  3390 3390\n",
      "Unique words:  1200\n",
      "Overall sentiment:  Sentiment(polarity=0.06807116415781189, subjectivity=0.46131531767895456)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.437227404203412"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_weights_for_words_in_sentences(sentences):\n",
    "    allblobs = TextBlob('. '.join(sentences))\n",
    "\n",
    "    total_words = len(allblobs.words)\n",
    "    total_unique = len(allblobs.word_counts)\n",
    "    all_term_counts = np.array(allblobs.word_counts.values())\n",
    "    print \"All words: \", total_words, all_term_counts.sum()\n",
    "    print \"Unique words: \", total_unique\n",
    "    print \"Overall sentiment: \", allblobs.sentiment\n",
    "    infos = 1 - np.log(all_term_counts*1.0 + 1.0)  / np.log(all_term_counts.sum() + 1.0)\n",
    "    info_dict = dict(zip(allblobs.word_counts.keys(), infos.tolist()))\n",
    "        \n",
    "    return info_dict\n",
    "    \n",
    "def get_weights_for_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "#     transformer = TfidfTransformer()\n",
    "    \n",
    "    counts_matrix = vectorizer.fit_transform(sentences)\n",
    "    all_term_counts = np.diff(sp.csc_matrix(counts_matrix, copy=False).indptr)\n",
    "    infos = 1 - np.log(all_term_counts*1.0 + 1.0)  / np.log(all_term_counts.sum() + 1.0)\n",
    "    print \"total words\"\n",
    "    print all_term_counts.sum()\n",
    "    info_dict = {w: infos[i] for i, w in enumerate(vectorizer.get_feature_names())}\n",
    "    return info_dict\n",
    "    \n",
    "\n",
    "info_dict = get_weights_for_sentences([s['text'] for s in all_articles[0]['sentences'].values()])\n",
    "count_dict = get_weights_for_words_in_sentences([s['text'] for s in all_articles[0]['sentences'].values()])\n",
    "count_frame = pd.DataFrame.from_dict(count_dict, orient='index')\n",
    "frame = pd.DataFrame.from_dict(info_dict, orient='index')\n",
    "count_dict.get('to', \"not found\")\n",
    "# This shows the difference between the info dict obtained from\n",
    "# text blob and from sklearn\n",
    "# count_frame.sort_values(by=0, axis='index').ix[:11],\\\n",
    "# frame.sort_values(by=0, axis='index').ix[:11]\n",
    "\n",
    "# This shows how the sklearn tokenizer is worse than textblobs...\n",
    "# Well, not worse, but it finds way less words, and different words...\n",
    "# Lets use the same tokenizer (textblob)\n",
    "# print \"Count dict differences\"\n",
    "# for x in count_dict:\n",
    "#     if x not in info_dict:\n",
    "#         print \"{0} not in dict\".format(x.encode('utf-8'))\n",
    "\n",
    "# print \"\\n\\nInfo dict differences\"\n",
    "# for x in info_dict:\n",
    "#     if x not in count_dict:\n",
    "#         print \"{0} not in dict\".format(x.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "from textblob.wordnet import VERB, NOUN, ADJ, ADV\n",
    "import re\n",
    "\n",
    "re_noun = re.compile('.*N[NPR].*')\n",
    "re_adj = re.compile('.*JJ.*')\n",
    "re_verb = re.compile('.*(VB|BE|DO|HV).*')    \n",
    "re_adv = re.compile('.*W?RB.*')\n",
    "WORDNET_TRESH = 0.20\n",
    "\n",
    "def pos_to_wordnetpos(pos):\n",
    "    '''    \n",
    "    NN or NP or NR = NOUN\n",
    "    JJ = ADJECTIVE\n",
    "    VB or BE or DO or HV = VERB\n",
    "    WRB or RB = ADVERB\n",
    "    '''\n",
    "    if re_noun.search(pos):\n",
    "        return NOUN\n",
    "    elif re_adj.search(pos):\n",
    "        return ADJ\n",
    "    elif re_verb.search(pos):\n",
    "        return VERB\n",
    "    elif re_adv.search(pos):\n",
    "        return ADV\n",
    "    else:\n",
    "        return None\n",
    "def word_to_synset(pos_tagged_word):\n",
    "    '''    \n",
    "    NN or NP or NR = NOUN\n",
    "    JJ = ADJECTIVE\n",
    "    VB or BE or DO or HV = VERB\n",
    "    WRB or RB = ADVERB\n",
    "    '''\n",
    "    w = pos_tagged_word[0]    \n",
    "    pos = pos_to_wordnetpos(pos_tagged_word[1])\n",
    "    return dissambiguate_synset(w, pos)\n",
    "    \n",
    "def dissambiguate_synset(word, wordnet_pos):\n",
    "    synset = Word(w).get_synsets(pos)\n",
    "    if 'list' in synset.__class__:\n",
    "        return synset[0]\n",
    "    else:\n",
    "        return synset\n",
    "    \n",
    "pairs = all_pruned_pairs[0]\n",
    "test_article_sentences = [s['text'] for s in all_articles[0]['sentences'].values()]\n",
    "weight_dict = get_weights_for_words_in_sentences(\n",
    "    test_article_sentences\n",
    ")\n",
    "\n",
    "s1 = all_articles[0]['sentences'][pairs[0][0]]['text']\n",
    "s2 = all_articles[0]['sentences'][pairs[0][1]]['text']\n",
    "print \"Sentence1:\\n{0}\\nSentence2:\\n{1}\".format(s1,s2)\n",
    "\n",
    "blob1 = TextBlob(s1)\n",
    "blob2 = TextBlob(s2)\n",
    "\n",
    "# get the sentence POS tags and create the wordnet objects\n",
    "tagged_words1 = {w[0]: pos_to_wordnetpos(w[1]) for w in blob1.tags}\n",
    "tagged_words2 = {w[0]: pos_to_wordnetpos(w[1]) for w in blob2.tags}\n",
    "\n",
    "# create word set with unique words, and convert it to list for iteration\n",
    "synsets1 = {word_to_synset(w) for w in blob1.tags}\n",
    "synsets2 = {word_to_synset(w) for w in blob2.tags}\n",
    "words_corpus = list(synsets1 + synsets2)\n",
    "\n",
    "# function to find similarity of word with set of words\n",
    "def similarity_with_words(synset1, blob, tag_dict):\n",
    "    max_sim = 0\n",
    "    most_similar_w = ''\n",
    "    for w in blob.words:\n",
    "        synset2 = dissambiguate_synset(w, tag_dict[w])\n",
    "        \n",
    "        if synset1 == synset2:\n",
    "            max_s = 1\n",
    "            most_similar_w = w\n",
    "            break\n",
    "            \n",
    "        wordnet_sim = synset1.path_similarity(synset2)\n",
    "        wordnet_sim = wordnet_sim if wordnet_sim > WORDNET_TRESH else 0\n",
    "\n",
    "        if wordnet_sim > max_sim:\n",
    "            max_sim = wordnet_sim\n",
    "            most_similar_w = w\n",
    "    \n",
    "    # weight similarity using tfidf (or I can use real word frequencies)\n",
    "    weight = weight_dict[most_similar_w.lower()]\n",
    "    max_sim *= w\n",
    "    \n",
    "    return max_sim\n",
    "        \n",
    "# form similarity vectors\n",
    "s1 = []\n",
    "s2 = []\n",
    "for i,synset in enumerate(words_corpus):\n",
    "    # note that I should have the synsets in my corpus...\n",
    "    s1.append(similarity_with_words(synset, blob1, tagged_words1))\n",
    "    s2.append(similarity_with_words(synset, blob2, tagged_words2))\n",
    "\n",
    "print \"Similarity Vectors\"\n",
    "frame = pd.DataFrame(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.8, subjectivity=0.75) Sentiment(polarity=-0.6000000000000001, subjectivity=0.7666666666666666)\n"
     ]
    }
   ],
   "source": [
    "example_pos = TextBlob(\"THIS IS GREAT YEEEEEEESSSS\")\n",
    "example_nen = TextBlob(\"FUCK YOU MAN, I HATE YOU with all my fucking body. \")\n",
    "\n",
    "print example_pos.sentiment,example_nen.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
